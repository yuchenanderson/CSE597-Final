{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4z-AlSoLcsk"
      },
      "outputs": [],
      "source": [
        "!unzip Codes.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6PwDIQw92yo"
      },
      "outputs": [],
      "source": [
        "!pip uninstall transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsfzrtPZ6zwk"
      },
      "outputs": [],
      "source": [
        "!pip install --no-cache-dir transformers sentencepiece\n",
        "from T5_WSD import T5_WSD\n",
        "wsd = T5_WSD()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc6zLn5GfD06"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTLw93lLSyki"
      },
      "outputs": [],
      "source": [
        "!mkdir Temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFPabofHR2x6"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1byX4wpe1UjyCVyYrT04sW17NnycKAK7N -O semeval-2023-task-1-V-WSD-train-v1.zip\n",
        "!unzip semeval-2023-task-1-V-WSD-train-v1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFYfJ5MsS-nS"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm wiktionaryparser\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9CQQkSonCDS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyJPOkb2S_FH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from os.path import exists\n",
        "\n",
        "import json\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
        "print(\"here\")\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device: \", device)\n",
        "\n",
        "from pprint import pprint as pprint\n",
        "from typing import List, Optional\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "#%%\n",
        "import torch\n",
        "import clip\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "Image.MAX_IMAGE_PIXELS = 1000000000\n",
        "#%%\n",
        "from collections import Counter\n",
        "#%%\n",
        "from nltk.corpus import wordnet as wn\n",
        "from wiktionaryparser import WiktionaryParser\n",
        "#%%\n",
        "CLIP_MODEL = \"ViT-B/32\"\n",
        "dictionary_type = 'compensate' # GPT_gen (DG or CADG), compensate (WN+DG or WN+CADG), wordnet (WN)\n",
        "GPT_def_path = 'Definitions/GPT_Context_Definitions.json' # definition path\n",
        "# GPT_def_path = 'Definitions/GPT_Definitions.json' # definition path\n",
        "data_path = \"../Experimental Codes/Dataset/VWSD\" # data path\n",
        "\n",
        "d_split = 'train'\n",
        "#%%\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CLIP_model, preprocess = clip.load(CLIP_MODEL, device=device)\n",
        "#%%\n",
        "def image_loader(path, preprocessor):\n",
        "    img_files = os.listdir(path)\n",
        "\n",
        "    imgs = {}\n",
        "    for file in tqdm(img_files):\n",
        "        file_path = os.path.join(path, file)\n",
        "        #img = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
        "        img = preprocess(Image.open(file_path)).unsqueeze(0)\n",
        "        imgs[file] = img\n",
        "    return imgs\n",
        "#%%\n",
        "'''\n",
        "if d_split == 'trial':\n",
        "    image_path = data_path+\"/trial/all_images\"\n",
        "    data_file_path = data_path+\"/trial/trial.data.txt\"\n",
        "    gold_file_path = data_path+\"/trial/trial.gold.txt\"\n",
        "    image_dict_path = 'Temp/img_dict_trial.pkl'\n",
        "\n",
        "elif d_split == 'train':\n",
        "    image_path = data_path+\"/train/train_v1/train_images_v1\"\n",
        "    data_file_path = data_path+\"/train/train_v1/train.data.v1.txt\"\n",
        "    gold_file_path = data_path+\"/train/train_v1/train.gold.v1.txt\"\n",
        "   '''\n",
        "if d_split == 'trial':\n",
        "    image_path = \"semeval-2023-task-1-V-WSD-train-v1/trial_v1/all_images\"\n",
        "    data_file_path = \"semeval-2023-task-1-V-WSD-train-v1/trial_v1/trial.data.txt\"\n",
        "    gold_file_path = \"semeval-2023-task-1-V-WSD-train-v1/trial_v1/trial.gold.txt\"\n",
        "    image_dict_path = 'Temp/img_dict_trial.pkl'\n",
        "\n",
        "elif d_split == 'train':\n",
        "    image_path = \"semeval-2023-task-1-V-WSD-train-v1/train_v1/train_images_v1\"\n",
        "    data_file_path = \"semeval-2023-task-1-V-WSD-train-v1/train_v1/train.data.v1.txt\"\n",
        "    gold_file_path = \"semeval-2023-task-1-V-WSD-train-v1/train_v1/train.gold.v1.txt\"\n",
        "    image_dict_path = 'Temp/img_dict_train.pkl'\n",
        "\n",
        "\n",
        "\n",
        "if os.path.isfile(image_dict_path):\n",
        "    img_dict = pickle.load(open(image_dict_path,'rb'))\n",
        "    print(\"here1\")\n",
        "else:\n",
        "    print(\"here2\")\n",
        "    img_dict = image_loader(image_path,preprocess)\n",
        "    pickle.dump(img_dict, open(image_dict_path,'wb'))\n",
        "    print(\"here3\")\n",
        "#%%\n",
        "\n",
        "#%%\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "#%%\n",
        "class GPT_definitions(object):\n",
        "    def __init__(self, GPT_def_path):\n",
        "        temp_dict = json.load(open(GPT_def_path))\n",
        "\n",
        "        GPT_dict = {}\n",
        "        for key in temp_dict.keys():\n",
        "            for k in temp_dict[key]:\n",
        "                 GPT_dict[k] = []\n",
        "        for key in temp_dict.keys():\n",
        "            for k in temp_dict[key]:\n",
        "                 GPT_dict[k].append(temp_dict[key][k])\n",
        "        self.GPT_dict = GPT_dict\n",
        "\n",
        "    def get_senses(self, target_word):\n",
        "        return self.GPT_dict[target_word]\n",
        "#%%\n",
        "class Dictionary_wrapper(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        #self.dictionary_type=dict_type\n",
        "\n",
        "        self.wn = wn\n",
        "        self.wiktionary_parser = WiktionaryParser()\n",
        "        self.GPT_definitions = GPT_definitions(GPT_def_path)\n",
        "\n",
        "    def get_wn_definitions(self, target_word):\n",
        "        sense_definitions = []\n",
        "        target_senses = self.wn.synsets(target_word)\n",
        "        for synset in target_senses:\n",
        "            #if synset.pos() == 'n':\n",
        "            sense_definition = synset.definition().split(';')[0]\n",
        "            sense_definitions.append(sense_definition)\n",
        "        sense_definitions = list(set(sense_definitions))\n",
        "\n",
        "        return sense_definitions\n",
        "\n",
        "    def get_wiktionary_definitions(self, target_word, lang):\n",
        "        parser = self.wiktionary_parser\n",
        "        sense_definitions = []\n",
        "\n",
        "        target_senses = parser.fetch(target_word, lang)\n",
        "        #print(target_senses)\n",
        "        for synset in target_senses:\n",
        "            #print(synset)\n",
        "            for polysemy in synset['definitions']:\n",
        "                #print(definition)\n",
        "                for sense in polysemy['text'][1:]:\n",
        "                    sense_definition = sense.split(';')[0]\n",
        "                    #print(sense_definition)\n",
        "                sense_definitions.append(sense_definition)\n",
        "        sense_definitions = list(set(sense_definitions))\n",
        "\n",
        "        return sense_definitions\n",
        "\n",
        "    def get_GPT_definitions(self, target_word, lang):\n",
        "        return self.GPT_definitions.get_senses(target_word)\n",
        "\n",
        "    def get_definitions(self, target_word, dictionary_type = \"wordnet\", lang='english'):\n",
        "        # dictionary: wordnet, wiktionary, both\n",
        "        #print(dictionary_type)\n",
        "        if dictionary_type == 'wordnet':\n",
        "            sense_definitions = self.get_wn_definitions(target_word)\n",
        "        elif dictionary_type == 'GPT_gen':\n",
        "            sense_definitions = self.get_GPT_definitions(target_word, lang)\n",
        "        # elif dictionary_type == 'both':\n",
        "        #     sense_definitions = self.get_wn_definitions(target_word)\n",
        "        #     sense_definitions += self.get_GPT_definitions(target_word, lang)\n",
        "        elif dictionary_type == 'compensate':\n",
        "            sense_definitions = self.get_wn_definitions(target_word)\n",
        "            if len(sense_definitions) == 0:\n",
        "                sense_definitions += self.get_GPT_definitions(target_word, lang)\n",
        "        return sense_definitions\n",
        "#%%\n",
        "\n",
        "#%%\n",
        "dictionary = Dictionary_wrapper()\n",
        "#%%\n",
        "def data_loader(data_file_path, dictionary, dictionary_type=\"wordnet\", gold_file_path = None):\n",
        "\n",
        "    def target_word_preprocessing(target_word):\n",
        "        #target_word = target_word.replace('-',' ')\n",
        "        return target_word\n",
        "\n",
        "\n",
        "    text_data = {}\n",
        "\n",
        "    fin_data = open(data_file_path)\n",
        "    candidate_lens = []\n",
        "    for data_index, line in tqdm(enumerate(fin_data)):\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "\n",
        "        cols = line.split('\\t')\n",
        "        target_word = cols[0]; target_word = target_word_preprocessing(target_word)\n",
        "        context = cols[1]\n",
        "        candidates = cols[2:]\n",
        "\n",
        "        #sense_definitions = []\n",
        "        #target_senses = wn.synsets(target_word)\n",
        "        sense_definitions = dictionary.get_definitions(target_word, dictionary_type)\n",
        "        wordnet_definitions = dictionary.get_definitions(target_word, 'wordnet')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        text_data[data_index] = {'target_word': target_word,\n",
        "                                 'sense_definitions': sense_definitions,\n",
        "                                 'wordnet_definitions': wordnet_definitions,\n",
        "                                 'context': context,\n",
        "                                 'candidates': candidates}\n",
        "\n",
        "        candidate_lens.append(len(candidates))\n",
        "    fin_data.close()\n",
        "\n",
        "\n",
        "    if gold_file_path:\n",
        "        fin_gold = open(gold_file_path)\n",
        "        for gold_index, line in enumerate(fin_gold):\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "\n",
        "            gold = line\n",
        "            text_data[gold_index]['gold'] = gold\n",
        "    print(np.mean(candidate_lens))\n",
        "    return text_data\n",
        "#%%\n",
        "text_data = data_loader(data_file_path,\n",
        "                        dictionary,\n",
        "                        dictionary_type,\n",
        "                        gold_file_path = gold_file_path)\n",
        "#%%\n",
        "text_data_keys = list(text_data.keys())\n",
        "random.shuffle(text_data_keys)\n",
        "text_data_keys = text_data_keys\n",
        "text_data = {key: text_data[key] for key in text_data_keys}\n",
        "#%%\n",
        "# text_data\n",
        "#%%\n",
        "\n",
        "#%%\n",
        "class VWSD_CLIP_Zeroshot(object):\n",
        "    def __init__(self, CLIP_model, CLIP_preprocess):\n",
        "        self.CLIP_model = CLIP_model;\n",
        "        self.CLIP_preprocess = CLIP_preprocess\n",
        "\n",
        "    def test(self, context, images):\n",
        "        CLIP_model = self.CLIP_model\n",
        "        CLIP_preprocess = self.CLIP_preprocess\n",
        "\n",
        "        text = clip.tokenize([context]).to(device)\n",
        "        images = torch.stack(images).squeeze().to(device)\n",
        "\n",
        "        image_features = CLIP_model.encode_image(images)\n",
        "        text_features = CLIP_model.encode_text(text)\n",
        "\n",
        "        logits_per_image, logits_per_text = CLIP_model(images, text)\n",
        "\n",
        "\n",
        "    def evaluate_posterior(self, text_data, img_dict):\n",
        "        # I <- candidate images, T <- context, a <- ambiguous\n",
        "        # P(I|T) ~ P(I,T)/ <- CLIP\n",
        "        # 56.5% Accuracy 10% random\n",
        "        CLIP_model = self.CLIP_model\n",
        "\n",
        "        preds = []\n",
        "        golds = []\n",
        "        answers = []\n",
        "        partial_answers = []\n",
        "        for data_index in tqdm(text_data.keys()):\n",
        "            data = text_data[data_index]\n",
        "            context = data['context']; candidates = data['candidates']\n",
        "            target_word = data['target_word']\n",
        "            context = context.replace(target_word, '\\\"'+target_word+'\\\"')\n",
        "\n",
        "            gold = data['gold']; gold_index = data['candidates'].index(gold)\n",
        "\n",
        "            text = clip.tokenize([context]).to(device)\n",
        "            with torch.no_grad():\n",
        "                images = [img_dict[candidate] for candidate in candidates]\n",
        "                images = torch.stack(images).squeeze().to(device)\n",
        "                image_features = CLIP_model.encode_image(images)\n",
        "                text_features = CLIP_model.encode_text(text)\n",
        "\n",
        "                logits_per_image, logits_per_text = CLIP_model(images, text)\n",
        "                probs = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
        "                pred = np.argmax(probs[0])\n",
        "\n",
        "                preds.append(data['candidates'][pred])\n",
        "                golds.append(gold)\n",
        "                if pred == gold_index:\n",
        "                    answers.append(1)\n",
        "                else:\n",
        "                    answers.append(0)\n",
        "\n",
        "                sorted_indexes = reversed(np.argsort(probs[0]))\n",
        "\n",
        "                i = 1\n",
        "                #print(sorted_indexes)\n",
        "                for index in sorted_indexes:\n",
        "                    #print(index, gold_index)\n",
        "                    if index == gold_index:\n",
        "                        #partial_answers = 1/i\n",
        "                        partial_answers.append(1/i)\n",
        "                        break\n",
        "                    i+=1\n",
        "        return preds, golds, answers, partial_answers\n",
        "\n",
        "\n",
        "    def evaluate_bayesian_posterior(self, text_data, img_dict):\n",
        "        # P(I|T) -> \\sigma \\simga P(I|D,T)P(D|T)\n",
        "        # 75%\n",
        "        CLIP_model = self.CLIP_model\n",
        "\n",
        "        preds = []\n",
        "        golds = []\n",
        "        answers = []\n",
        "        partial_answers = []\n",
        "        #probs = []\n",
        "        for data_index in tqdm(text_data.keys()):\n",
        "            data = text_data[data_index]\n",
        "            context = data['context']; candidates = data['candidates']\n",
        "            target_word = data['target_word']\n",
        "            context = context.replace(target_word, '\\\"'+target_word+'\\\"')\n",
        "\n",
        "            sense_definitions = data['sense_definitions']\n",
        "            sense_definitions = [context + ' : ' + sense_definition for sense_definition in sense_definitions]\n",
        "\n",
        "            if not len(sense_definitions):\n",
        "                #print('no sense')\n",
        "                sense_definitions += [context]\n",
        "\n",
        "\n",
        "            gold = data['gold']; gold_index = data['candidates'].index(gold)\n",
        "\n",
        "\n",
        "\n",
        "            #text = clip.tokenize([context]).to(device)\n",
        "            with torch.no_grad():\n",
        "                context_text = clip.tokenize([context], truncate = True).to(device)\n",
        "                definition_text = clip.tokenize(sense_definitions, truncate = True).to(device)\n",
        "\n",
        "                images = [img_dict[candidate] for candidate in candidates]\n",
        "                images = torch.stack(images).squeeze().to(device)\n",
        "\n",
        "                # 1 answer and 9 distractors\n",
        "                image_features = CLIP_model.encode_image(images)\n",
        "                text_features = CLIP_model.encode_text(context_text)\n",
        "                # 4 senses in wordnet [4X512]\n",
        "                def_features = CLIP_model.encode_text(definition_text)\n",
        "\n",
        "\n",
        "\n",
        "                # probs text to def\n",
        "                # P(D_i|T)\n",
        "                # [1X4]\n",
        "                logits_per_definition = torch.matmul(text_features, def_features.T)\n",
        "                prob_dist_definitions = logits_per_definition.softmax(dim=-1)\n",
        "\n",
        "                # P(I|T,D)\n",
        "                # [4 X 10]\n",
        "                logits_per_image, logits_per_text = CLIP_model(images, definition_text)\n",
        "                probs_per_image = logits_per_image.softmax(dim=-1)\n",
        "                probs_per_text = logits_per_text.softmax(dim=-1)\n",
        "\n",
        "                bayesian_probs = torch.matmul(prob_dist_definitions, probs_per_text).cpu().numpy()\n",
        "                pred = np.argmax(bayesian_probs)\n",
        "\n",
        "                sorted_indexes = reversed(np.argsort(bayesian_probs[0]))\n",
        "\n",
        "                i = 1\n",
        "                for index in sorted_indexes:\n",
        "                    if index == gold_index:\n",
        "                        #partial_answers = 1/i\n",
        "                        partial_answers.append(1/i)\n",
        "                        break\n",
        "                    i+=1\n",
        "                #ranks = [data['candidates'][index] for index in sorted_indexes]\n",
        "\n",
        "                preds.append(data['candidates'][pred])\n",
        "                golds.append(gold)\n",
        "                if pred == gold_index:\n",
        "                    answers.append(1)\n",
        "                else:\n",
        "                    answers.append(0)\n",
        "        return preds, golds, answers, partial_answers\n",
        "\n",
        "#%%\n",
        "VWSD_CLIP = VWSD_CLIP_Zeroshot(CLIP_model, preprocess)\n",
        "#%%\n",
        "p_preds, p_golds, p_answers, p_partial_answers =  VWSD_CLIP.evaluate_posterior(text_data, img_dict)\n",
        "print(\"Accuracy:\", \"%.2f\"%(np.mean(p_answers)*100))\n",
        "print(\"MRR:\", \"%.2f\"%(np.mean(p_partial_answers)*100))\n",
        "#%%\n",
        "index = 0\n",
        "p_sense_nums_w = []\n",
        "p_sense_nums_r = []\n",
        "for t, p, g in zip(text_data, p_preds, p_golds):\n",
        "    if p != g:\n",
        "        #print(t, text_data[t]['context'], p, g, len(text_data[t]['sense_definitions']))\n",
        "        p_sense_nums_w.append(len(text_data[t]['wordnet_definitions']))\n",
        "    else:\n",
        "        p_sense_nums_r.append(len(text_data[t]['wordnet_definitions']))\n",
        "    index+=1\n",
        "\n",
        "print(sorted(Counter(p_sense_nums_w).items()))\n",
        "print(sorted(Counter(p_sense_nums_r).items()))\n",
        "\n",
        "right_when_zero = sorted(Counter(p_sense_nums_r).items())[0][1]\n",
        "wrong_when_zero = sorted(Counter(p_sense_nums_w).items())[0][1]\n",
        "\n",
        "right_when_one = sorted(Counter(p_sense_nums_r).items())[1][1]\n",
        "wrong_when_one = sorted(Counter(p_sense_nums_w).items())[1][1]\n",
        "\n",
        "right_when_over_one = 0\n",
        "wrong_when_over_one = 0\n",
        "\n",
        "for s, c in sorted(Counter(p_sense_nums_w).items()):\n",
        "    if s > 1: wrong_when_over_one += c\n",
        "for s, c in sorted(Counter(p_sense_nums_r).items()):\n",
        "    if s > 1: right_when_over_one += c\n",
        "\n",
        "print('Hits@1 |D^t|==0: %.2f'%(right_when_zero/(right_when_zero + wrong_when_zero)*100))\n",
        "print('Hits@1 |D^t|==1: %.2f'%(right_when_one/(right_when_one + wrong_when_one)*100))\n",
        "print('Hits@1 |D^t|>1: %.2f'%(right_when_over_one/(right_when_over_one + wrong_when_over_one)*100))\n",
        "#%%\n",
        "sum(p_answers)\n",
        "#%%\n",
        "bp_preds, bp_golds, bp_answers, bp_partial_answers =  VWSD_CLIP.evaluate_bayesian_posterior(text_data, img_dict)\n",
        "print(\"Accuracy:\", \"%.2f\"%(np.mean(bp_answers)*100))\n",
        "print(\"MRR:\", \"%.2f\"%(np.mean(bp_partial_answers)*100))\n",
        "#%%\n",
        "index = 0\n",
        "pb_sense_nums_w = []\n",
        "pb_sense_nums_r = []\n",
        "for t, p, g in zip(text_data, bp_preds, bp_golds):\n",
        "    if p != g:\n",
        "        #print(t, text_data[t]['context'], p, g, len(text_data[t]['sense_definitions']))\n",
        "        pb_sense_nums_w.append(len(text_data[t]['wordnet_definitions']))\n",
        "    else:\n",
        "        pb_sense_nums_r.append(len(text_data[t]['wordnet_definitions']))\n",
        "    index+=1\n",
        "print(sorted(Counter(pb_sense_nums_w).items()))\n",
        "print(sorted(Counter(pb_sense_nums_r).items()))\n",
        "\n",
        "right_when_zero = sorted(Counter(pb_sense_nums_r).items())[0][1]\n",
        "wrong_when_zero = sorted(Counter(pb_sense_nums_w).items())[0][1]\n",
        "\n",
        "right_when_one = sorted(Counter(pb_sense_nums_r).items())[1][1]\n",
        "wrong_when_one = sorted(Counter(pb_sense_nums_w).items())[1][1]\n",
        "\n",
        "right_when_over_one = 0\n",
        "wrong_when_over_one = 0\n",
        "\n",
        "for s, c in sorted(Counter(pb_sense_nums_w).items()):\n",
        "     if s > 1: wrong_when_over_one += c\n",
        "for s, c in sorted(Counter(pb_sense_nums_r).items()):\n",
        "     if s > 1: right_when_over_one += c\n",
        "\n",
        "print('Hits@1 |D^t|==0: %.2f'%(right_when_zero/(right_when_zero + wrong_when_zero)*100))\n",
        "print('Hits@1 |D^t|==1: %.2f'%(right_when_one/(right_when_one + wrong_when_one)*100))\n",
        "print('Hits@1 |D^t|>1: %.2f'%(right_when_over_one/(right_when_over_one + wrong_when_over_one)*100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}